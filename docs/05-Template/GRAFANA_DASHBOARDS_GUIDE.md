# GuÃ­a: Dashboards Grafana por Sprint

**VersiÃ³n:** 1.0  
**Fecha:** 2025-10-16  
**Contexto:** Plantilla v2.4 Enterprise

---

## ğŸ¯ Objetivo

Documentar claramente **quÃ© sprints necesitan dashboards Grafana** y **cuÃ¡les no**, evitando bloat innecesario mientras se garantiza observabilidad operacional.

---

## ğŸ“Š Matriz de DecisiÃ³n por Sprint

| Sprint | Dashboard | Criterios Cumplidos | JustificaciÃ³n |
|--------|-----------|---------------------|---------------|
| **S0 - Bootstrap** | âŒ | 0/4 | Solo setup - no hay mÃ©tricas runtime aÃºn |
| **S1 - Contratos** | âŒ | 0/4 | ValidaciÃ³n estÃ¡tica - no afecta runtime |
| **S2 - Realtime Engine** | âœ…âœ… | 4/4 | Latencia p95, concurrencia, SLOs crÃ­ticos |
| **S3 - Batch Processing** | âœ…âœ… | 4/4 | Throughput, queue depth, async behavior |
| **S4 - Streaming SSE** | âœ…âœ… | 4/4 | Active clients, backpressure, drops |
| **S5 - Model Management** | âœ… | 3/4 | Swaps, loads, memory, latency por modelo |
| **S6 - Providers** | âœ…âœ… | 4/4 | Latencia por provider, error rates, multi-provider |
| **S7 - Cache** | âš ï¸ | 2/4 | **Opcional** - puede usar mÃ©tricas de S2/S6 |
| **S8 - Rate Limiting** | âŒ | 1/4 | Alertas simples suficientes - no requiere dashboard |
| **S9 - Observability** | âœ…âœ…âœ… | 4/4 | **Sprint maestro** - consolida todos los dashboards |
| **S10 - Performance** | âœ…âœ… | 4/4 | Benchmarks, L1-L3 cache, pool utilization |
| **S10.5 - Perf Framework** | âœ…âœ… | 4/4 | OptimizaciÃ³n sistemÃ¡tica, comparaciÃ³n A/B |

---

## ğŸ” Criterios de DecisiÃ³n (â‰¥2 para incluir Dashboard)

### 1. âœ… Introduce MÃ©tricas Runtime
**Â¿El sprint genera mÃ©tricas operacionales en producciÃ³n?**

- **SÃ**: Latencia, throughput, concurrencia, memoria, CPU
- **NO**: ConfiguraciÃ³n, contratos estÃ¡ticos, documentaciÃ³n

**Ejemplos:**
- âœ… S2: `inference_latency_p95`, `concurrent_requests`
- âœ… S4: `sse_active_clients`, `sse_dropped_events`
- âŒ S1: ValidaciÃ³n de schemas YAML (no produce mÃ©tricas runtime)

---

### 2. âœ… Cambia Comportamiento Observable
**Â¿El sprint modifica cÃ³mo se comporta el sistema en producciÃ³n?**

- **SÃ**: Streaming, batch, async, pooling, caching
- **NO**: Refactoring interno, documentaciÃ³n, tests

**Ejemplos:**
- âœ… S3: Cambia de sync a batch async
- âœ… S4: Agrega streaming SSE (nuevo patrÃ³n observable)
- âŒ S8: Rate limiting es transparente al usuario

---

### 3. âœ… Afecta SLOs
**Â¿El sprint tiene objetivos de performance cuantificables?**

- **SÃ**: p95 < Xms, error rate < Y%, throughput > Z req/s
- **NO**: Funcionalidad correcta sin SLOs especÃ­ficos

**Ejemplos:**
- âœ… S2: SLO p95 < 200ms
- âœ… S6: SLO error rate < 1% por provider
- âŒ S1: No tiene SLOs de runtime (solo validaciÃ³n)

---

### 4. âœ… Requiere Debug Operacional
**Â¿Los logs son suficientes para debuggear issues en producciÃ³n?**

- **SÃ necesita dashboard**: CorrelaciÃ³n temporal, percentiles, multi-tenant
- **NO necesita dashboard**: Errores puntuales, trazas lineales

**Ejemplos:**
- âœ… S4: "Â¿Por quÃ© se estÃ¡n dropeando eventos SSE?" â†’ Necesita correlaciÃ³n entre `queue_size` y `active_clients`
- âœ… S6: "Â¿QuÃ© provider estÃ¡ fallando?" â†’ Dashboard con breakdown por provider
- âŒ S8: "Â¿Se estÃ¡ rate-limitando?" â†’ Log simple + alerta basta

---

## ğŸ“ˆ AnÃ¡lisis Detallado por Sprint

### âŒ S0 - Bootstrap
**Criterios:** 0/4

- âŒ **Runtime**: No hay mÃ©tricas aÃºn
- âŒ **Observable**: Solo estructura de cÃ³digo
- âŒ **SLOs**: No aplica
- âŒ **Debug**: No hay nada que debuggear

**DecisiÃ³n:** NO incluir dashboard

---

### âŒ S1 - Contratos
**Criterios:** 0/4

- âŒ **Runtime**: ValidaciÃ³n estÃ¡tica (CI/test time)
- âŒ **Observable**: No afecta comportamiento en producciÃ³n
- âŒ **SLOs**: ValidaciÃ³n binaria (pasa/falla)
- âŒ **Debug**: Errores claros en tests

**DecisiÃ³n:** NO incluir dashboard

---

### âœ…âœ… S2 - Realtime Engine
**Criterios:** 4/4

- âœ… **Runtime**: `inference_latency_ms`, `concurrent_requests`, `queue_depth`
- âœ… **Observable**: Sync request/response con concurrencia
- âœ… **SLOs**: p95 < 200ms, error rate < 0.5%
- âœ… **Debug**: Necesita correlaciÃ³n latencia vs concurrencia

**MÃ©tricas clave:**
- `inference_request_latency_ms` (histogram)
- `inference_concurrent_requests` (gauge)
- `inference_queue_depth` (gauge)
- `inference_errors_total` (counter)

**Dashboard:** `deploy/grafana/dashboards/s2-realtime.json`

---

### âœ…âœ… S3 - Batch Processing
**Criterios:** 4/4

- âœ… **Runtime**: `batch_throughput_rps`, `batch_queue_depth`, `batch_processing_time`
- âœ… **Observable**: Cambia de sync a batch async
- âœ… **SLOs**: Throughput > 1000 req/s, p95 < 500ms
- âœ… **Debug**: Cuellos de botella en queue

**MÃ©tricas clave:**
- `batch_requests_total` (counter)
- `batch_queue_depth` (gauge)
- `batch_processing_duration_seconds` (histogram)
- `batch_failed_requests_total` (counter)

**Dashboard:** `deploy/grafana/dashboards/s3-batch.json`

---

### âœ…âœ… S4 - Streaming SSE
**Criterios:** 4/4

- âœ… **Runtime**: `sse_active_clients`, `sse_queue_size`, `sse_dropped_events`
- âœ… **Observable**: Streaming long-lived connections
- âœ… **SLOs**: Drops < 0.1%, backpressure < 80%
- âœ… **Debug**: CorrelaciÃ³n drops vs active_clients vs queue_size

**MÃ©tricas clave:**
- `inference_sse_active_clients` (gauge)
- `inference_sse_queue_size` (histogram)
- `inference_sse_dropped_events_total` (counter)
- `inference_sse_event_latency_ms` (histogram)

**Dashboard:** `ops/grafana/dashboards/sse_overview.json` (ya existe en S4)

---

### âœ… S5 - Model Management
**Criterios:** 3/4

- âœ… **Runtime**: `model_swap_total`, `model_load_latency`, `model_memory_bytes`
- âœ… **Observable**: Swaps A/B, warm-up, eviction
- âœ… **SLOs**: Load latency < 2s, swap sin drops
- âŒ **Debug**: Relativamente simple (solo load/unload)

**MÃ©tricas clave:**
- `enis_inference_model_swap_total` (counter)
- `enis_inference_model_load_total` (counter)
- `enis_provider_invoke_latency_ms` (histogram) - por modelo
- `enis_inference_model_errors_total` (counter)

**Dashboard:** `docs/observability/s5-model-mgmt-dashboard.json` (ya existe en S5)

---

### âœ…âœ… S6 - Providers
**Criterios:** 4/4

- âœ… **Runtime**: Latencia y errores **por provider** (OpenAI, Anthropic, Groq, OSS)
- âœ… **Observable**: Multi-provider routing, fallback
- âœ… **SLOs**: p95 por provider, error rate < 1%
- âœ… **Debug**: "Â¿QuÃ© provider estÃ¡ fallando?" requiere breakdown

**MÃ©tricas clave:**
- `provider_requests_total{provider,model}` (counter)
- `provider_errors_total{provider,code}` (counter)
- `provider_latency_seconds{provider,model}` (histogram)
- `tokens_consumed_total{provider,model,tenant}` (counter)

**Dashboard:** `monitoring/grafana/dashboards/s6-providers.json` (mencionado en S6)

---

### âš ï¸ S7 - Cache (CONDICIONAL)
**Criterios:** 2/4

- âœ… **Runtime**: `cache_hit_ratio`, `cache_latency_ms`
- âš ï¸ **Observable**: Mejora latencia pero es transparente
- âœ… **SLOs**: Hit ratio > 70%, latency reduction > 50%
- âŒ **Debug**: Logs + mÃ©tricas de S2 pueden bastar

**DecisiÃ³n:** **OPCIONAL** - Puede usar paneles de S2 agregando:
- `inference_cache_hit_ratio` (gauge)
- `inference_cache_miss_ratio` (gauge)
- `inference_cache_latency_ms` (histogram)

**Alternativa:** Agregar 1-2 paneles al dashboard de S2 en lugar de dashboard completo

---

### âŒ S8 - Rate Limiting
**Criterios:** 1/4

- âœ… **Runtime**: `rate_limit_exceeded_total`
- âŒ **Observable**: Transparente (429 errors)
- âŒ **SLOs**: Binario (permite/rechaza)
- âŒ **Debug**: Log simple + alerta basta

**DecisiÃ³n:** NO incluir dashboard - alertas simples suficientes

**MÃ©tricas recomendadas:**
- `rate_limit_exceeded_total{tenant,tier}` (counter)
- Alerta: `rate(rate_limit_exceeded_total[5m]) > 10`

---

### âœ…âœ…âœ… S9 - Observability (CRÃTICO)
**Criterios:** 4/4 + Sprint Maestro

- âœ… **Runtime**: Todas las mÃ©tricas anteriores
- âœ… **Observable**: Consolida observabilidad completa
- âœ… **SLOs**: SLOs globales del sistema
- âœ… **Debug**: Dashboard unificado + drill-down

**DecisiÃ³n:** **OBLIGATORIO** - Este sprint:
1. Consolida todos los dashboards de S2-S6
2. Agrega dashboard de overview general
3. Provisiona alertas
4. Configura logging/tracing

**Dashboards:**
- `observability/dashboards/enis-overview.json` (general)
- Upgrade de dashboards S2-S6 con drill-downs
- Dashboard de SLOs

---

### âœ…âœ… S10 - Performance
**Criterios:** 4/4

- âœ… **Runtime**: Benchmarks, L1-L3 cache, pool utilization
- âœ… **Observable**: Optimizaciones medibles
- âœ… **SLOs**: Mejoras cuantificables (p95, throughput)
- âœ… **Debug**: ComparaciÃ³n before/after

**MÃ©tricas clave:**
- MÃ©tricas de S2-S6 con optimizaciones
- `l1_cache_hit_ratio`, `l2_cache_hit_ratio`, `l3_cache_hit_ratio`
- `db_pool_utilization`
- `request_budget_cost_usd`

**Dashboard:** Dashboard de performance con comparaciÃ³n temporal

---

### âœ…âœ… S10.5 - Performance Framework
**Criterios:** 4/4

- âœ… **Runtime**: Framework de benchmarking continuo
- âœ… **Observable**: OptimizaciÃ³n sistemÃ¡tica A/B
- âœ… **SLOs**: Targets de mejora por categorÃ­a
- âœ… **Debug**: ComparaciÃ³n entre variantes

**Dashboard:** Ya implementado en S10.5 con:
- Variables `tenant`, `tier`
- Thresholds por panel
- ComparaciÃ³n A/B

---

## ğŸ¨ Tipos de Dashboards

### 1. Dashboard de Feature (S2-S6, S10)
**PropÃ³sito:** Monitorear funcionalidad especÃ­fica

**Estructura:**
- 4-6 paneles clave
- Variables: tenant, tier, instance
- Thresholds alineados a SLOs del sprint

**Ejemplos:**
- S2: Realtime Engine (latencia, concurrencia)
- S4: Streaming SSE (clients, drops)
- S6: Providers (latency por provider)

---

### 2. Dashboard de Overview (S9)
**PropÃ³sito:** Vista general del sistema

**Estructura:**
- 8-12 paneles consolidados
- Drill-down a dashboards especÃ­ficos
- SLOs globales
- Alerting guidance

---

### 3. Dashboard de Performance (S10+)
**PropÃ³sito:** ComparaciÃ³n temporal y A/B

**Estructura:**
- MÃ©tricas before/after
- ComparaciÃ³n de variantes
- AnÃ¡lisis de cuellos de botella
- Cost tracking

---

## ğŸ“ Plantilla: DecisiÃ³n de Dashboard

Cuando crees un nuevo sprint, usa este checklist:

```markdown
## Â¿Incluir Dashboard Grafana?

### Criterios (necesita â‰¥2 âœ…):
- [ ] **Runtime**: Â¿Genera mÃ©tricas operacionales? (latencia, throughput, concurrencia)
- [ ] **Observable**: Â¿Cambia comportamiento en producciÃ³n? (streaming, batch, async)
- [ ] **SLOs**: Â¿Tiene objetivos cuantificables? (p95 < Xms, error rate < Y%)
- [ ] **Debug**: Â¿Logs no son suficientes? (necesita correlaciÃ³n, percentiles)

### DecisiÃ³n:
- **âœ… SÃ incluir dashboard** (â‰¥2 criterios) â†’ Usar plantilla v2.4 completa
- **âŒ NO incluir dashboard** (<2 criterios) â†’ Omitir secciÃ³n de Grafana
- **âš ï¸ OPCIONAL** (2 criterios justos) â†’ Evaluar si agregar paneles a dashboard existente

### Alternativas si NO:
- Usar mÃ©tricas de sprint anterior (ej: S7 usa S2/S6)
- Configurar alertas simples sin dashboard
- Diferir a S9 (Observability sprint)
```

---

## ğŸš€ ImplementaciÃ³n

### Para incluir Dashboard en un sprint:

1. **Usar secciÃ³n completa de plantilla v2.4:**
   ```markdown
   ## ğŸ“Š Dashboard Grafana Provisionado (v2.4)
   
   [Incluir JSON completo con paneles especÃ­ficos del sprint]
   ```

2. **Definir mÃ©tricas especÃ­ficas:**
   - Nomenclatura: `{service}_{feature}_{metric}_total`
   - Labels: `tenant`, `tier`, `model`, etc.
   - Tipos: Counter, Gauge, Histogram, Summary

3. **Configurar thresholds por SLO:**
   ```json
   "thresholds": {
     "steps": [
       { "color": "green", "value": null },
       { "color": "yellow", "value": 800 },  // Warning
       { "color": "red", "value": 1200 }     // Critical
     ]
   }
   ```

4. **Crear ConfigMap K8s:**
   ```yaml
   apiVersion: v1
   kind: ConfigMap
   metadata:
     name: grafana-dashboard-s[n]
   data:
     s[n]-dashboard.json: |
       [JSON del dashboard]
   ```

### Para NO incluir Dashboard:

1. **Omitir secciÃ³n de Grafana de la plantilla**

2. **Documentar alternativa en "Monitoring":**
   ```markdown
   ## ğŸ“Š Monitoring
   
   Este sprint NO requiere dashboard dedicado porque [razÃ³n].
   
   **Alternativas:**
   - Usar mÃ©tricas de dashboard S[X]
   - Configurar alertas simples: [ejemplos]
   - Diferir observabilidad detallada a S9
   ```

---

## ğŸ“š Referencias

- **Plantilla v2.4:** `docs/03-Template/SPRINT_TEMPLATE_CLAUDE_CLI.md`
- **Sprint S4 (ejemplo completo):** `docs/01-sprint/S4/S4-engine_streaming_sse.md` (lÃ­neas 2592-2613)
- **Sprint S10.5 (performance):** `docs/01-sprint/S10.5/s_10_5_performance_framework_optimizacion_sistematica_v_2.md`
- **Grafana Provisioning:** https://grafana.com/docs/grafana/latest/administration/provisioning/

---

## âœ… Checklist Final

Antes de crear un sprint, confirma:

- [ ] Â¿He evaluado los 4 criterios de decisiÃ³n?
- [ ] Â¿He decidido si incluir dashboard o no?
- [ ] Si SÃ: Â¿He definido las mÃ©tricas clave (â‰¥4)?
- [ ] Si SÃ: Â¿He configurado thresholds por SLO?
- [ ] Si NO: Â¿He documentado alternativa de monitoring?
- [ ] Â¿He verificado que otros sprints no cubren ya esto?

---

**Ãšltima actualizaciÃ³n:** 2025-10-16  
**VersiÃ³n:** 1.0  
**Mantenedor:** Platform Engineering Team

